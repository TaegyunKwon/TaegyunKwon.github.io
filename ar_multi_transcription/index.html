<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="Project Page for ISMIR Procedings" />
        <meta name="author" content="Taegyun Kwon" />
        <title>Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">ISMIR 2020 Project Page</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">Menu<i class="fas fa-bars ml-1"></i></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#demo">Demo</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#results">Results</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Masthead-->
        <header class="masthead">
            <div class="container">
                <div class="row">
                    <div class="col">
                        <div class="masthead-heading" style="font-size: 40px; color:#fd7e14;">
                            Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model
                        </div>
                        <div class="masthead-subheading" style="font-size: 2.0rem;">
                            <a> Taegyun Kwon, Dasaem Jeong and Juhan Nam</a>
                        </div>
                        <div class="masthead-subheading" style="font-style:normal;">
                            <a> Graduate School of Culture Technology, KAIST, South Korea</a>
                        </div>
                        <div class="masthead-subheading" style="font-style:normal; font-size: 1.2rem;">
                            <a> International Society for Music Information Retrieval Conference (ISMIR), 2020</a>
                        </div>
                    </div>
                </div>
            </div>
        </header>


        <!-- Abstract -->
        <section class="page-section bg-light" id="Abstract">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Abstract</h2>
                    <div class="body">
                        Recent  advances  in  polyphonic  piano  transcription  have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multipleloss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order  is  learned  by  an  auto-regressive  connection  within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters.
                    </div>
                </div>
            </div>
        </section>

        <!-- Demo -->
        <section class="page-section bg-light" id="demo">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Demo</h2>
                    <iframe width="720" height="400" style="display:block; margin:0 auto;" src="https://www.youtube.com/embed/pBqBArcXRto">
                    </iframe>
                    <p align='center'> 
                        Synchrozied transciption result. Transcribed midi is playback by Disklavier. </br> SeungJin Cho: Chopin Scherzo in B flat minor Op. 31 
                        <a href="https://www.youtube.com/watch?v=iliNPUB9GSA">[Original Source]</a>
                    </p>
                </div>
            </div>
        </section>
        <!-- Summary -->
        <section class="page-section bg-light" id="Summary">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Summary</h2>
                    <div class="body">
                        We explored neural-network architecture for piano transcription. 
                        We focused that frame-by-frame prediction could be easier if the prior note states are known.
                        To apply this, we explicitly informs the model by frame of the previous note state.
                        We also examined various representation type to represent note states.
                        <br>Our main contributions are as follows:
                        <ul>
                            <li>We proposed autoregressive transcription model, which is online, simple and general architecture. We showed that the model reflect sequential dependency and have similar perfomance capacity compare to SOTA models</li>
                            <li>We showed that represent multiple note states with softmax and predict them with single network does not degrade the performance</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>
        

        <!-- Motivation -->
        <section class="page-section bg-light" id="Motivation">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Motivation</h2>
                    <div class="body">
                    </div>
                </div>
            </div>
        </section>

        <!-- results -->
        <section class="page-section" id="results">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">알고리즘</h2>

                    <video height='400', controls>
                        <source src="assets/mov/spectrogram_crop.mp4" alt='Your browser does not support the video tag.' type="video/mp4">
                    </video> 
                    <p align='center'>조성진 Scherzo 연주 일부의 스펙트로그램. Generated using <a href=https://musiclab.chromeexperiments.com/Spectrogram/> Chrome musiclab </a></p>

                    <p align="left" style="font-size:20px;margin-top:30px;">
                        우리가 듣는 모든 음악, 모든 소리는 공기의 떨림입니다. 빠르게 떨리는 소리는 높은소리, 느리게 떨리는 소리는 낮은 소리로 인식되며 
                        스펙트로그램을 통해 소리의 주파수 별 성분을 보면, 어떤 소리가 나는지 대략적으로 알 수 있습니다.
                        예를들어, 위에 있는 스펙트로그램을 보면 소리를 듣지 않아도 낮은 두번의 음들이 나오고, 조금씩 올라가는 높은 음들이 크게 나올 것이라는 것을 알 수 있습니다. 
                    </p>    

                    <p align="left" style="font-size:20px;margin-top:30px;">
                        하지만 실제로 우리가 듣는 것들은 더 복잡합니다. 위 짧은 영상에서도 많은 음표가 울리고 있고,
                        만약 숙련된 음악가라면 어떤음을 쳤는지, 얼마나 세게, 얼마나 짧게 쳤는지와 같은 것들을 들을 수 있을것입니다.
                        위에서 연주된 정확한 음들을 피아노 건반을 따라 표시하면 아래와 같이 됩니다 (색깔은 세기를 나타냅니다)
                    </p>    

                    <img src="assets/img/piano_roll.png", height="400">
                    <p align='center'>조성진 Scherzo 연주 일부의 피아노 롤</p>

                    <p align="left" style="font-size:20px;margin-top:30px;">
                        우리는 딥러닝을 이용해서 위의 스펙트로그램으로 부터 연주된 음표들을 구별해내는 알고리즘을 학습시켰습니다.
                        연주된 음표의 음고, 세기, 길이를 정확히 예측한다면 이 피아노 롤을 다시 피아노를 통해 연주하면 원래 연주와 아주 유사한 연주를 재현 할 수 있게 됩니다.
                        위의 데모들은, 이렇게 예측된 피아노 롤에 페달을 추가로 예측하여 자동연주 피아노를 통해 재현한 영상들 입니다.
                        자세한 알고리즘은 아래의 논문을 참조해 주세요.
                    </p>    

                    <h3 class="section-subheading" align="left" style="font-size:30px;margin-top:30px;">Future</h3>
                    <p align="left" style="font-size:20px;margin-top:-50px;">
                        연주 정보를 담은 피아노 롤은 연주를 재연하는 데 쓰이는 것 뿐 아니라 연주자가 구체적으로 어떻게 연주를 했는지
                        분석할 수 있게 해줍니다. 연주 정보를 모아 좋은 연주를 구성하는 요소는 무엇인지, 연주의 스타일이란 어떻게
                        구성되는지 실마리를 찾는 작업이 다음 단계이며, 궁극적으로는 음악 연주에서 발휘되는 인간의 창의성을 이해하고
                        인간과 인공지능이 함께 연주할 수 있는 미래를 향하고 있습니다.
                    </p>

                    <h3 class="section-subheading" align="left" style="font-size:30px;margin-top:30px;">Contact</h3>
                    <p align="left" style="font-size:20px;margin-top:-50px;">
                        권태균 ilcobo2@gmail.com
                    </p>
                    <h3 class="section-subheading" align="left" style="font-size:30px;margin-top:30px;">Acknowledgement</h3>
                    <p align="left" style="font-size:20px;margin-top:-50px;">
                        이 연구는 삼성전자 미래기술육성센터의 ICT 창의과제 지원을 받아 이루어졌습니다. (SRFC-IT1702-12)
                    </p>

                    <p align="left" style="font-size:20px;">
                        과학적 출판에 인용하는 경우에는 ISMIR 2020에 출판될 "POLYPHONIC PIANO TRANSCRIPTION USING AUTOREGRESSIVEMULTI-STATE NOTE MODEL" 
                        논문을 인용해주시면 감사하겠습니다. (링크 추가 예정)
                    </p>
                </div>
            </div>
        </section>
        
        <!-- Conclusion -->
        <section class="page-section bg-light" id="Conclusion">
            Conclusion
        </section>

        
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Contact form JS-->
        <script src="assets/mail/jqBootstrapValidation.js"></script>
        <script src="assets/mail/contact_me.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>