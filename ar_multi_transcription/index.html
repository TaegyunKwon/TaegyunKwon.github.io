<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="Project Page for ISMIR Procedings" />
        <meta name="author" content="Taegyun Kwon" />
        <title>Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">Home</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">Menu<i class="fas fa-bars ml-1"></i></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#demo">Demo</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#results">Results</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Masthead-->
        <header class="masthead">
            <div class="container">
                <div class="row">
                    <div class="col">
                        <div class="masthead-heading" style="font-size: 2.5rem; color:#fd7e14;">
                            Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model
                        </div>
                        <div class="masthead-subheading" style="font-size: 2.0rem; line-height: 2.0rem;">
                            <a> Taegyun Kwon, Dasaem Jeong and Juhan Nam</a>
                        </div>
                        <div class="masthead-subheading" style="font-style:normal;">
                            <a> Graduate School of Culture Technology, KAIST, South Korea</a>
                        </div>
                        <div class="masthead-subheading" style="font-style:normal; font-size: 1.2rem;">
                            <a> International Society for Music Information Retrieval Conference (ISMIR), 2020</a>
                        </div>
                    </div>
                </div>
            </div>
        </header>


        <!-- Abstract -->
        <section class="page-section bg-light" id="Abstract">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Abstract</h2>
                    <div class="lead">
                        Recent  advances  in  polyphonic  piano  transcription  have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multipleloss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order  is  learned  by  an  auto-regressive  connection  within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters.
                    </div>
                </div>
            </div>
        </section>

        <!-- Demo -->
        <section class="page-section bg-light" id="demo">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Demo</h2>
                        <div class="embed-responsive embed-responsive-16by9">
                            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/pBqBArcXRto"></iframe>
                        </div>
                        <figcaption class="figure-caption text-center">
                            Synchrozied transciption result. Audio is playback of transcribed midi by Disklavier. </br> SeungJin Cho: Chopin Scherzo in B flat minor Op. 31 
                            <a href="https://www.youtube.com/watch?v=iliNPUB9GSA">[Original Source]</a>
                        </figcaption>
                </div>
            </div>
        </section>
        <!-- Summary -->
        <section class="page-section bg-light" id="Summary">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Summary</h2>
                    <div class="lead">
                        We explored neural-network architecture for piano transcription. 
                        We focused that frame-by-frame prediction could be easier if the prior note states are known.
                        To apply this, we explicitly informs the model by frame of the previous note state.
                        We also examined various representation type to represent note states.
                        <br>Our main contributions are as follows:
                        <ul>
                            <li>We proposed autoregressive transcription model, which is online, simple and general architecture. We showed that the model reflect sequential dependency and have similar perfomance capacity compare to SOTA models</li>
                            <li>We showed that representing multiple note states with softmax and predict them with single network does not degrade the performance</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>
        

        <!-- Motivation -->
        <section class="page-section bg-light" id="Motivation">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Motivation</h2>
                    <div class="lead">
                        Most of NN based transciption algorithms are based on frame-by-frame model, which predict note activation at every frame.
                        <figure class="figure">
                            <img src="assets/img/frame.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center">frame-by-frame prediction model</figcaption>
                        </figure>
                        However, when each frame prediction is independently predicted, resulting posteriogram often contains blurry region. 
                        <img src="assets/img/blurry.PNG" class="rouned mx-auto d-block" style="max-width: 70%; height: auto;"></img>
                        <figcaption class="figure-caption text-center">prediction example</figcaption>
                        
                        To overcome this problem, several methods were proposed, including post-processing with musical language model 
                        [<a href="#ref3">3</a>, <a href="#ref5">5</a>] or GAN based regularization[<a href="#ref4">4</a>].
                        We thought that thoses blurry region indicates uncentainty, and the decision would be easier if the model take account the situation of notes just before.
                        For example, in the following clip it is hard to transcribe notes if you hear only middle part of notes, but it becomes much easier if you listen it from beginning and take account which notes were played.
                        <div class="embed-responsive embed-responsive-16by9 mx-auto d-block" style="max-width: 70%; height: auto;">
                            <iframe class="embed-responsive-item" src="assets/mov/motivation.mp4"></iframe>
                        </div>
                        <figcaption class="figure-caption text-center">State conditioning example</figcaption>
                        We end up with the model with auto-regressive connection, which predict frame activation not only based on spectrogram but also previous note states.
                        <br>
                        Also, we also had to decide representation of note states. Since it is critial to employing addtional note states [<a href="#ref1">1</a>],
                        we also tried to adapt additional states (onset, offset, <a href="#reonset">re-onset</a>). Previous works usually represent multiple note states with multi binary labels
                        with branched network structure [<a href="#ref1">1</a>, <a href="#ref3">3</a>], but we tried to represent all states with a single softmax, since they can be
                        regarded as mutually-exclusive, related class.

                    </div>
                </div>
            </div>
        </section>

        <!-- Methods -->
        <section class="page-section" id="methods">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Methods</h2>
                    <h3 class="section-subheading ">Auto-regressive model</h3>
                    <div class="lead">
                        Our proposed model follows stacked CNN-RNN architecture, similar to onsets and frames [<a href="#ref1">1</a>].
                        In our model, the previous note states are connatenated with CNN output, and feeded into RNN layers.
                        <figure class="figure">
                            <img src="assets/img/AR-model.PNG" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center">Abstract diagram of auto-regressive model</figcaption>
                        </figure>
                        <figure class="figure">
                            <img src="assets/img/model-v3_3-07.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center"> Unrolled model diagram at frame t. x indicates acoustic feature (mel-spectrogram) and y indicates label</figcaption>
                        </figure>

                    </div>
                    <h3 class="section-subheading ">Multi-State representation</h3>
                    <div class="lead">
                        We tested five kinds of note state representations. From binary to five states, we subdivide the classes into more classes.
                        Especially, we added re-onset class, which is special case of onset while note is sustained.
                        <figure class="figure">
                            <img src="assets/img/state_notation_v2_4-02.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center"> Unrolled model diagram at frame t. x indicates acoustic feature (mel-spectrogram) and y indicates label</figcaption>
                        </figure>

                        <figure class="figure">
                            <img src="assets/img/representation.PNG" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center"> Unrolled model diagram at frame t. x indicates acoustic feature (mel-spectrogram) and y indicates label</figcaption>
                        </figure>

                    </div>
                    <video height='400', controls>
                        <source src="assets/mov/spectrogram_crop.mp4" alt='Your browser does not support the video tag.' type="video/mp4">
                    </video> 
                    <p align='center'>조성진 Scherzo 연주 일부의 스펙트로그램. Generated using <a href=https://musiclab.chromeexperiments.com/Spectrogram/> Chrome musiclab </a></p>

                    <p align="left" style="font-size:20px;margin-top:30px;">
                        우리가 듣는 모든 음악, 모든 소리는 공기의 떨림입니다. 빠르게 떨리는 소리는 높은소리, 느리게 떨리는 소리는 낮은 소리로 인식되며 
                        스펙트로그램을 통해 소리의 주파수 별 성분을 보면, 어떤 소리가 나는지 대략적으로 알 수 있습니다.
                        예를들어, 위에 있는 스펙트로그램을 보면 소리를 듣지 않아도 낮은 두번의 음들이 나오고, 조금씩 올라가는 높은 음들이 크게 나올 것이라는 것을 알 수 있습니다. 
                    </p>    

                    <p align="left" style="font-size:20px;margin-top:30px;">
                        하지만 실제로 우리가 듣는 것들은 더 복잡합니다. 위 짧은 영상에서도 많은 음표가 울리고 있고,
                        만약 숙련된 음악가라면 어떤음을 쳤는지, 얼마나 세게, 얼마나 짧게 쳤는지와 같은 것들을 들을 수 있을것입니다.
                        위에서 연주된 정확한 음들을 피아노 건반을 따라 표시하면 아래와 같이 됩니다 (색깔은 세기를 나타냅니다)
                    </p>    

                    <img src="assets/img/piano_roll.png", height="400">
                    <p align='center'>조성진 Scherzo 연주 일부의 피아노 롤</p>

                    <p align="left" style="font-size:20px;margin-top:30px;">
                        우리는 딥러닝을 이용해서 위의 스펙트로그램으로 부터 연주된 음표들을 구별해내는 알고리즘을 학습시켰습니다.
                        연주된 음표의 음고, 세기, 길이를 정확히 예측한다면 이 피아노 롤을 다시 피아노를 통해 연주하면 원래 연주와 아주 유사한 연주를 재현 할 수 있게 됩니다.
                        위의 데모들은, 이렇게 예측된 피아노 롤에 페달을 추가로 예측하여 자동연주 피아노를 통해 재현한 영상들 입니다.
                        자세한 알고리즘은 아래의 논문을 참조해 주세요.
                    </p>    

                    <h3 class="section-subheading" align="left" style="font-size:30px;margin-top:30px;">Future</h3>
                    <p align="left" style="font-size:20px;margin-top:-50px;">
                        연주 정보를 담은 피아노 롤은 연주를 재연하는 데 쓰이는 것 뿐 아니라 연주자가 구체적으로 어떻게 연주를 했는지
                        분석할 수 있게 해줍니다. 연주 정보를 모아 좋은 연주를 구성하는 요소는 무엇인지, 연주의 스타일이란 어떻게
                        구성되는지 실마리를 찾는 작업이 다음 단계이며, 궁극적으로는 음악 연주에서 발휘되는 인간의 창의성을 이해하고
                        인간과 인공지능이 함께 연주할 수 있는 미래를 향하고 있습니다.
                    </p>

                    <h3 class="section-subheading" align="left" style="font-size:30px;margin-top:30px;">Contact</h3>
                    <p align="left" style="font-size:20px;margin-top:-50px;">
                        권태균 ilcobo2@gmail.com
                    </p>
                    <h3 class="section-subheading" align="left" style="font-size:30px;margin-top:30px;">Acknowledgement</h3>
                    <p align="left" style="font-size:20px;margin-top:-50px;">
                        이 연구는 삼성전자 미래기술육성센터의 ICT 창의과제 지원을 받아 이루어졌습니다. (SRFC-IT1702-12)
                    </p>

                    <p align="left" style="font-size:20px;">
                        과학적 출판에 인용하는 경우에는 ISMIR 2020에 출판될 "POLYPHONIC PIANO TRANSCRIPTION USING AUTOREGRESSIVEMULTI-STATE NOTE MODEL" 
                        논문을 인용해주시면 감사하겠습니다. (링크 추가 예정)
                    </p>
                </div>
            </div>
        </section>

        <!-- Reference -->
        <section class="page-section bg-light" id="Conclusion">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Reference</h2>
                    <div class="lead" style="font-size: 1rem;">
                    <ul>
                        <li id="ref1">
                            [1] Curtis Hawthorne, Erich Elsen,
                                Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev
                                Oore, Douglas Eck. “Onsets and Frames: Dual-Objective Piano Transcription”, 19th International Society for Music Information Retrieval
                                Conference, Paris, France, 2018.

                        </li>
                        <li id="ref2">
                            [2] S. Böck and M. Schedl, “Polyphonic piano note transcription  with  recurrent  neural  networks,”  in Proc.of the 37th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2012,pp. 121–124
                        </li>
                        <li id="ref3">
                            [3] R. Kelz, S. Böck, and G. Widmer, “Deep polyphonic ADSR piano note transcription,” in Proc. of the 44th International Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, May 2019, pp.246–250
                        </li>
                        <li id="ref4">
                            [4]  J. Kim and J. Bello, “Adversarial learning for improved onsets and frames music transcription,” in Proc. of 20th International Society for Music Information Retrieval Conference (ISMIR), 2019, pp. 670–677
                        </li>
                        <li id="ref5">
                            [5]  A.  Ycart,  A.  McLeod,  E.  Benetos,  and  K.  Yoshii, “Blending acoustic and language model predictions for automatic  music  transcription,”  in Proc. of the 20th International Society for Music Information Retrieval Conference (ISMIR), 2019, pp. 454–461
                        </li>
                    </ul>
                </div>

                </div>
            </div>
        </section>
        
        <!-- Conclusion -->
        <section class="page-section bg-light" id="Conclusion">
            Conclusion
        </section>

        
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Contact form JS-->
        <script src="assets/mail/jqBootstrapValidation.js"></script>
        <script src="assets/mail/contact_me.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>