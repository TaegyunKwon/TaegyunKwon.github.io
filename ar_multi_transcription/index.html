<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="Project Page for ISMIR Procedings" />
        <meta name="author" content="Taegyun Kwon" />
        <title>Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">Home</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">Menu<i class="fas fa-bars ml-1"></i></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Abstract">Abstract</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Demo">Demo</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Summary">Summary</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Motivation">Motivation</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Methods">Methods</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Results">Results</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Reference">Referece</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Masthead-->
        <header class="masthead">
            <div class="container">
                <div class="row">
                    <div class="col">
                        <div class="masthead-heading" style="font-size: 2.5rem; color:#fd7e14;">
                            Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model
                        </div>
                        <div class="masthead-subheading" style="font-size: 2.0rem; line-height: 2.0rem;">
                            <a> Taegyun Kwon, Dasaem Jeong and Juhan Nam</a>
                        </div>
                        <a href="https://mac.kaist.ac.kr" target="_blank">
                            <img src="assets/img/logo_final.png" class="img-fluid" style="height: 3.0rem;"></img>
                        </a>
                        <div class="masthead-subheading" style="font-style:normal;">
                            <a> Graduate School of Culture Technology, KAIST, South Korea</a>
                        </div>
                        <div class="masthead-subheading" style="font-style:normal; font-size: 1.2rem;">
                            <a> International Society for Music Information Retrieval Conference (ISMIR), 2020</a>
                        </div>
                    </div>
                </div>
                <a class="btn btn-primary btn-lx text-uppercase" href="https://arxiv.org/abs/2010.01104" target="_blank">Paper</a>
                <a class="btn btn-primary btn-lx text-uppercase" href="https://program.ismir2020.net/poster_3-17.html" target="_blank">Poster</a>
                <a class="btn btn-primary btn-lx text-uppercase" href="https://github.com/jdasam/online_amt" target="_blank">Realtime Repo</a>
            </div>
        </header>
    </body>

    <body id="contents">
        <!-- Abstract -->
        <section class="page-section bg-light" id="Abstract">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Abstract</h2>
                    <div class="lead">
                        Recent  advances  in  polyphonic  piano  transcription  have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multipleloss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order  is  learned  by  an  auto-regressive  connection  within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters.
                    </div>
                </div>
            </div>
        </section>

        <!-- Demo -->
        <section class="page-section bg-light" id="Demo">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Demo</h2>
                        <div class="embed-responsive embed-responsive-16by9">
                            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/pBqBArcXRto"></iframe>
                        </div>
                        <figcaption class="figure-caption text-center">
                            Synchrozied transciption result. Audio is playback of transcribed midi by Disklavier. </br> SeungJin Cho: Chopin Scherzo in B flat minor Op. 31 
                            <a href="https://www.youtube.com/watch?v=iliNPUB9GSA">[Original Source]</a>
                        </figcaption>
                        <div class="embed-responsive embed-responsive-16by9">
                            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/jWP4PESkmR8"></iframe>
                        </div>
                        <figcaption class="figure-caption text-center">
                            Realtime Demo of the system. Played by the Me(Taegyun) and my dear colleagues(Sangeon and Eunjin @ MACLAB).
                        </figcaption>
                </div>
            </div>
        </section>

        <!-- Summary -->
        <section class="page-section bg-light" id="Summary">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Summary</h2>
                    <div class="lead">
                        We explored neural-network architecture for piano transcription. 
                        We focused that frame-by-frame prediction could be easier if the prior note states are known.
                        To apply this, we explicitly informs the model by frame of the previous note state.
                        We also examined various representation type to represent note states.
                        <br>Our main contributions are as follows:
                        <ul>
                            <li>We proposed autoregressive transcription model, which is online, simple and general architecture. We showed that the model reflect sequential dependency and have similar perfomance capacity compare to SOTA models</li>
                            <li>We showed that representing multiple note states with softmax and predict them with single network does not degrade the performance</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Motivation -->
        <section class="page-section bg-light" id="Motivation">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Motivation</h2>
                    <div class="lead">
                        Most of NN based transciption algorithms are based on frame-by-frame model, which predict note activation at every frame.
                        <figure class="figure">
                            <img src="assets/img/frame_model.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center">Frame-by-frame prediction model</figcaption>
                        </figure>
                        However, when each frame prediction is independently predicted, resulting posteriogram often contains blurry region. 
                        <img src="assets/img/blurry.PNG" class="rouned mx-auto d-block" style="max-width: 70%; height: auto;"></img>
                        <figcaption class="figure-caption text-center">prediction example</figcaption>
                        
                        To overcome this problem, several methods were proposed, including post-processing with musical language model 
                        [<a href="#ref2">2</a>, <a href="#ref4">4</a>] or GAN based regularization[<a href="#ref3">3</a>].
                        We thought that thoses blurry region indicates uncentainty, and the decision would be easier if the model take account the situation of notes just before.
                        For example, in the following clip it is hard to transcribe notes if you hear only middle part of notes, but it becomes much easier if you listen it from beginning and take account which notes were played.
                        <div class="embed-responsive embed-responsive-16by9 mx-auto d-block" style="max-width: 70%; height: auto;">
                            <iframe controls="true" class="embed-responsive-item" src="assets/mov/motivation.mp4"></iframe>
                        </div>
                        <figcaption class="figure-caption text-center">State conditioning example</figcaption>
                        We end up with the model with auto-regressive connection, which predict frame activation not only based on spectrogram but also previous note states.
                        <br>
                        Also, we also had to decide representation of note states. Since it is critial to employing addtional note states [<a href="#ref1">1</a>],
                        we also tried to adapt additional states (onset, offset, <a href="#reonset">re-onset</a>). Previous works usually represent multiple note states with multi binary labels
                        with branched network structure [<a href="#ref1">1</a>, <a href="#ref2">2</a>], but we tried to represent all states with a single softmax, since they can be
                        regarded as mutually-exclusive, related class.

                    </div>
                </div>
            </div>
        </section>

        <!-- Methods -->
        <section class="page-section bg-light" id="Methods">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Methods</h2>
                    <h3 class="section-subheading ">Auto-Regressive Model</h3>
                    <div class="lead">
                        Our proposed model follows stacked CNN-RNN architecture, similar to onsets and frames [<a href="#ref1">1</a>].
                        In our model, the previous note states are connatenated with CNN output, and feeded into RNN layers.
                        <figure class="figure">
                            <img src="assets/img/AR-model.PNG" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center">Abstract diagram of auto-regressive model</figcaption>
                        </figure>
                        <figure class="figure">
                            <img src="assets/img/model-v3_3-07.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center"> Unrolled model diagram at frame t. x indicates acoustic feature (mel-spectrogram) and y indicates label</figcaption>
                        </figure>

                    </div>
                    <h3 class="section-subheading ">Multi-State Representation</h3>
                    <div class="lead">
                        We tested five kinds of note state representations. From binary to five states, we subdivide the classes into more classes.
                        Especially, we added re-onset class, which is special case of onset while the note is sustained.
                        <figure class="figure">
                            <img src="assets/img/state_notation_v2_4-02.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center"> multi-state representations</figcaption>
                        </figure>

                        Onsets and Frames[<a href="#ref1">1</a>] used two binary states and CNN-RNN stack for frame (note on) and onset, and   
                        Kelz et al [<a href="#ref2">2</a>] used three branches to express three binary states (on, onset, offset).
                        Compare to previous researches, we simplified network architecture by combining notes states into a single one-hot vector.
                        <figure class="figure">
                            <img src="assets/img/representation.PNG" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                            <figcaption class="figure-caption text-center"> abstact state representation comparision</figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </section>

        <!-- Results -->
        <section class="page-section bg-light" id="Results">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Results</h2>
                    <div class="lead">
                    We evaluate our model with MAESTRO dataset.
                    <br>
                    First of all, our model produce much clearer posteriogram compare to non-AR algorithms. It shows that the auto-regressive connection
                    helps the model to learn sequential dependency. However, our model also have drawbacks when it fails to capture offset; it tends to prolong notes too much.
                    <figure class="figure">
                        <img src="assets/img/frame_prob.PNG" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                        <figcaption class="figure-caption text-center"> frame-probability comparision</figcaption>
                    </figure>
                    

                    Comparison between AR and non-AR model shows it clearly affect note onset/offset predictions. Employing more note states also have positive effects,
                    but the difference wasn't that large in note onset.
                    <div class="row">
                        <div class="col">
                            <figure class="figure">
                                <img src="assets/img/results_ar.png" class="figure-img rouned mx-auto d-block" style="max-width: 100%; height: auto;">
                                <figcaption class="figure-caption text-center"> AR model comparison result</figcaption>
                            </figure>
                        </div>
                        <div class="col">
                        <figure class="figure">
                            <img src="assets/img/results_states.png" class="figure-img rouned mx-auto d-block" style="max-width: 100%; height: auto;">
                            <figcaption class="figure-caption text-center"> state representation comparision</figcaption>
                        </figure>
                        </div>
                    </div>

                    Employing re-onset states also seems to have positive effect on retrieve repeated notes 
                    <figure class="figure">
                        <img src="assets/img/reonset.PNG" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                        <figcaption class="figure-caption text-center"> detected onset activation. top: ground truth / middle: model with re-onset / bottom: model without reonset <figcaption>
                    </figure>
                    Our best model also achieved similar accuarcy compare to offline (bidirectional) onsets and frames model.
                    We think that the auto-regressive connection compansates lack of backward information.
                    <figure class="figure d-block">
                        <img src="assets/img/results_onf.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                        <figcaption class="figure-caption text-center"> reimplementation comparision</figcaption>
                    </figure>
                    We also tried beam-search decoding. Since high-dimensionality and multi-state of piano roll, it wasn't trivial to apply beam-search.
                    We proposed truncated pitch-wise beam-search, which only take account high-probable states and ignoring other pitches at time (see 3.5 in the paper). But it even always degrades the decoded results. We found that the model is poorly calibrated, which means that predicted frame-probability is not reliable.
                    <figure class="figure d-block">
                        <img src="assets/img/ece_together_v2-02.png" class="figure-img rouned mx-auto d-block" style="max-width: 70%; height: auto;">
                        <figcaption class="figure-caption text-center"> error calibration curve</figcaption>
                    </figure>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Conclusion -->
        <section class="page-section bg-light" id="Conclusion">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Conclusion</h2>
                    <div class="lead">
                        <ul>
                            <li>
                                The onset state  is  critical  to  improving  note  onset  scores and the offset and re-onset states help improving the note-with-offset score.
                            </li>
                            <li>
                                The auto-regressive MLM provides significantly higher accuracy on both note onset and offset estimation compared to its non-autoregressive version.
                            </li>
                            <li>
                                Our proposed model achieves transcription performance comparable to the state-of-the-art models evenwith the unidirectional RNN and fewer parameters.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Reference -->
        <section class="page-section bg-light" id="Reference">
            <div class="container">
                <div class="text-left">
                    <h2 class="section-heading text-uppercase">Reference</h2>
                    <div class="lead" style="font-size: 1rem;">
                    <ul>
                        <li id="ref1">
                            [1] Curtis Hawthorne, Erich Elsen,
                                Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev
                                Oore, Douglas Eck. “Onsets and Frames: Dual-Objective Piano Transcription”, 19th International Society for Music Information Retrieval
                                Conference, Paris, France, 2018.
                        </li>
                        <li id="ref2">
                            [2] R. Kelz, S. Böck, and G. Widmer, “Deep polyphonic ADSR piano note transcription,” in Proc. of the 44th International Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, May 2019, pp.246–250
                        </li>
                        <li id="ref3">
                            [3]  J. Kim and J. Bello, “Adversarial learning for improved onsets and frames music transcription,” in Proc. of 20th International Society for Music Information Retrieval Conference (ISMIR), 2019, pp. 670–677
                        </li>
                        <li id="ref4">
                            [4]  A.  Ycart,  A.  McLeod,  E.  Benetos,  and  K.  Yoshii, “Blending acoustic and language model predictions for automatic  music  transcription,”  in Proc. of the 20th International Society for Music Information Retrieval Conference (ISMIR), 2019, pp. 454–461
                        </li>
                    </ul>
                </div>
                </div>
            </div>
        </section>
        
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Contact form JS-->
        <script src="assets/mail/jqBootstrapValidation.js"></script>
        <script src="assets/mail/contact_me.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>